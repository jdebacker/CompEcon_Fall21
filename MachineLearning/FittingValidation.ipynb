{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model fitting and validation\n",
    "### by [Jason DeBacker](https://jasondebacker.com/), October 2021\n",
    "\n",
    "This notebook uses machine learning tools in Python's [`scikit-learn`](https://scikit-learn.org/stable/) package to illustrate model fitting and validation.  By validation, I'm referring to tests of the accuracy of the model outside of the sample from which is was fit (i.e., estimated).\n",
    "\n",
    "## 1. Fitting a model\n",
    "\n",
    "We begin with the PSID data we used in Problem Set #4.  We'll fit the same model prosed there (dropping the indicator for Hispanic ethnicity since there are not observations in the sample data of that ethnicity):\n",
    "\n",
    "$$\n",
    "ln(wage_{i,t}) = \\alpha + \\beta_1 Educ_{i,t} + \\beta_2 Age_{i,t} + \\beta_3 Age_{i,t}^2 + \\beta_4 Black_{i,t} + \\beta_5 OtherRace{i,t} + \\varepsilon_{i,t}\n",
    "$$\n",
    "\n",
    "Here, we fit the model using the `linear_models` module from the `scikit-learn` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jason.debacker/anaconda3/lib/python3.7/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Read in data and create variables\n",
    "data_path = os.path.join('..', 'Optimization', 'PS4_data.dta')\n",
    "psid = pd.read_stata(data_path,\n",
    "                     columns=['id68', 'year', 'hannhrs', 'hlabinc', 'hsex',\n",
    "                              'hyrsed', 'age', 'hrace'])\n",
    "\n",
    "# create wages and ln(wages)\n",
    "# note need to be careful with wages = 0\n",
    "psid['wage'] = psid['hlabinc']/psid['hannhrs']\n",
    "psid['ln_wage'] = np.log(psid['wage'])\n",
    "\n",
    "# create age squared\n",
    "psid['age_sq'] = psid['age'] ** 2\n",
    "\n",
    "# sample selection\n",
    "psid.drop(psid[psid.hsex != 1].index, inplace=True)\n",
    "psid.drop(psid[psid.age > 60].index, inplace=True)\n",
    "psid.drop(psid[psid.age < 25].index, inplace=True)\n",
    "psid.drop(psid[psid.wage < 7].index, inplace=True)\n",
    "psid.drop(psid[psid.wage == np.inf].index, inplace=True)\n",
    "\n",
    "# create dummy variables for race\n",
    "psid['black'] = (psid['hrace'] == 2).astype(int)\n",
    "psid['hispanic'] = (psid['hrace'] == 5).astype(int)\n",
    "psid['other'] = (\n",
    "    (psid['hrace'] == 3) | (psid['hrace'] == 4) |\n",
    "    (psid['hrace'] == 6) | (psid['hrace'] == 7)).astype(int)\n",
    "\n",
    "# drop obs if missing values for any variabls in regression model\n",
    "psid.dropna(axis=0,\n",
    "            subset=['ln_wage', 'hyrsed', 'age', 'age_sq', 'black', 'other'],\n",
    "            inplace=True)\n",
    "\n",
    "# add a constant\n",
    "psid['const'] = 1\n",
    "\n",
    "# keep just year 2000 so can compare to results from PS #4\n",
    "psid = psid[psid.year==2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [ 0.11033282  0.0843604  -0.00088677 -0.25961741 -0.0621439 ]\n",
      "Mean squared error: 0.29\n",
      "R-squared: 0.22\n"
     ]
    }
   ],
   "source": [
    "# Finally, estimate the model\n",
    "# Create linear regression object\n",
    "regr = LinearRegression()\n",
    "\n",
    "# Estimate the model using the full set of PSID data\n",
    "regr.fit(\n",
    "    psid[['hyrsed', 'age', 'age_sq', 'black', 'other']],  # the X's, it'll include a constant by default, but can change\n",
    "    psid['ln_wage']  # the y\n",
    ")\n",
    "\n",
    "# Get predicted values\n",
    "wage_pred = regr.predict(\n",
    "    psid[['hyrsed', 'age', 'age_sq', 'black', 'other']])\n",
    "               \n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.2f'\n",
    "      % sklearn.metrics.mean_squared_error(\n",
    "          psid['ln_wage'],  #actual values\n",
    "          wage_pred)  # predicted values\n",
    "     )\n",
    "# The R-sq:\n",
    "print('R-squared: %.2f'\n",
    "      % sklearn.metrics.r2_score(\n",
    "          psid['ln_wage'],  #actual values\n",
    "          wage_pred)  # predicted values\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Validation\n",
    "\n",
    "Now we'll consider some approaches to model validation.  That is, we'll see if our parameter estimates provide to give us accurate predictions, even on data we didn't train the model with.\n",
    "\n",
    "Recall from our lectures, we discussed 3 validation approaches:\n",
    "1. Validation data\n",
    "2. Leave on out cross-validation\n",
    "3. K-fold cross-validation\n",
    "\n",
    "We'll consider each of these in turn below.\n",
    "\n",
    "### 2.1. Validation data\n",
    "\n",
    "Validation data are data we set aside in order to test out model.  We do this as follows:\n",
    "\n",
    "1. Partition the data into a training set and a test set.\n",
    "2. Estimate the model using the training set.\n",
    "3. Evaluate the fit or predictive accuracy on the test set.\n",
    "\n",
    "The primary measure of fit is the mean squared error (MSE) of the estimated model on the test set. Let the test set have $n$ observations. The MSE of the test set is the sum of squared deviations of the actual dependent variable values minus the predicted values.\n",
    "\n",
    "$$\n",
    "MSE_{test} = \\frac{1}{n}\\sum_{i=1}^n\\left(y_i - \\hat{y}_i\\right)^2\n",
    "$$\n",
    "\n",
    "Let's employ the validation data approach on our PSID data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: partition the data\n",
    "# This function train_test_split is from sklearn.cross_validation\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "    psid[['hyrsed', 'age', 'age_sq', 'black', 'other']],   # the X's\n",
    "    psid['ln_wage'],  # the y's \n",
    "    test_size=0.4,  # use 60% of data to train, 40% to test\n",
    "    random_state=25  # can set the seed so can repoduce results\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: estimate the model on the training data\n",
    "# Create linear regression object\n",
    "regr = LinearRegression()\n",
    "# Estimate the model\n",
    "regr.fit(\n",
    "    X_train,  # the X's, it'll include a constant by default, but can change\n",
    "    y_train  # the y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error on the training data: 0.28\n",
      "Mean squared error on the test data: 0.30\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the accuracy of the model on\n",
    "# the validation data (and on the training\n",
    "# data for comparison)\n",
    "\n",
    "# get predicted values on training data\n",
    "pred_train = regr.predict(X_train)\n",
    "\n",
    "# get predicted values on test data\n",
    "pred_test = regr.predict(X_test)\n",
    "\n",
    "# The mean squared error\n",
    "print('Mean squared error on the training data: %.2f'\n",
    "      % sklearn.metrics.mean_squared_error(\n",
    "          y_train,  #actual values\n",
    "          pred_train)  # predicted values\n",
    "     )\n",
    "print('Mean squared error on the test data: %.2f'\n",
    "      % sklearn.metrics.mean_squared_error(\n",
    "          y_test,  #actual values\n",
    "          pred_test)  # predicted values\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, our fit is better on the training data than the test data!  But not by much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Leave-one-out cross validation\n",
    "\n",
    "Leave-one-out cross validation (LOOCV) were we create $n$ data sets each of length $n-1$ by dropping a single observation from our full dataset (once for each observation).  We estimate the model on the $n-1$ observations and then compute the squared error when we try to predict the dropped observation with our fitted model:\n",
    "\n",
    "$$\n",
    "SE_n = \\left(y_n - \\hat{f}_n(x_n)\\right)^2\n",
    "$$\n",
    "\n",
    "The LOOCV estimate for the test MSE is the average of these $n$ squared errors:\n",
    "\n",
    "$$\n",
    "MSE_{test} = \\frac{1}{n}\\sum_{i=1}^n SE_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOCV test MSE = 0.2866728662210982\n",
      "Std dev of the LOOCV test MSE = 0.5916542498151134\n"
     ]
    }
   ],
   "source": [
    "# create separate X and y so don't repeate DataFrame slices many times below\n",
    "Xvars = psid[['hyrsed', 'age', 'age_sq', 'black', 'other']].values\n",
    "yvars = psid['ln_wage'].values\n",
    "loo = sklearn.model_selection.LeaveOneOut()\n",
    "loo.get_n_splits(Xvars)\n",
    "\n",
    "# Create array to fill with squared errors\n",
    "n = psid.shape[0]\n",
    "SE_test = np.zeros(n)\n",
    "\n",
    "# Loop over all observations using the LeaveOneOut split method\n",
    "for train_index, test_index in loo.split(Xvars):\n",
    "    X_train, X_test = Xvars[train_index], Xvars[test_index]\n",
    "    y_train, y_test = yvars[train_index], yvars[test_index]\n",
    "    regr = sklearn.linear_model.LinearRegression()\n",
    "    regr.fit(X_train, y_train)\n",
    "    y_pred = regr.predict(X_test)\n",
    "    SE_test[test_index] = (y_test - y_pred) ** 2\n",
    "\n",
    "MSE_test = SE_test.mean()\n",
    "MSE_test_std = SE_test.std()\n",
    "print('LOOCV test MSE =', MSE_test)\n",
    "print('Std dev of the LOOCV test MSE =', MSE_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the test MSE is lower here than the validation data approach.  This is because we are using more data to train our model.\n",
    "\n",
    "\n",
    "### 2.3. k-fold cross validation\n",
    "\n",
    "$k$-fold cross validation is a method in which the dataset is randomly divided into $k$ groups (folds). We then set aside one of the folds to test our data on and use the observations in the other $k-1$ folds to train our data.  We then repeat this process $K$ times, leaving each of the $K$ folds out of the fiting process once.  Let $MSE_k$ be the MSE from the $k$th fold:\n",
    "\n",
    "$$\n",
    "MSE_{k} = \\frac{1}{n/k}\\sum_{j=1}^{n/k}(y_{j} - \\hat{f}_{k}(x_{j}))^2\n",
    "$$\n",
    "\n",
    "Then the $k$-fold estimate for the test MSE is the average of these $k$ test error estimates.\n",
    "\n",
    "$$\n",
    "MSE_{test} = \\frac{1}{k}\\sum_{i=1}^k MSE_i\n",
    "$$\n",
    "\n",
    "LOOCV is a special case of $k$-fold cross validation in which $k=n$.\n",
    "\n",
    "Let's implement this in Python:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the Titanic data again and test our logit model performance with a $k$-fold cross validation with $k=6$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOCV test MSE = 0.28684510590634776\n",
      "Std dev of the LOOCV test MSE = 0.044416467687342365\n"
     ]
    }
   ],
   "source": [
    "k = 20  # number of folds\n",
    "kf = sklearn.model_selection.KFold(\n",
    "    n_splits=k,\n",
    "    random_state=10,  # set seed so can reproduce results\n",
    "    shuffle=True\n",
    ")\n",
    "kf.get_n_splits(Xvars)\n",
    "\n",
    "# initialize array to fill with MSEs\n",
    "MSE_k = np.zeros(k)\n",
    "\n",
    "# Loop over the splits\n",
    "k_ind = int(0)\n",
    "for train_index, test_index in kf.split(Xvars):\n",
    "    X_train, X_test = Xvars[train_index], Xvars[test_index]\n",
    "    y_train, y_test = yvars[train_index], yvars[test_index]\n",
    "    regr = sklearn.linear_model.LinearRegression()\n",
    "    regr.fit(X_train, y_train)\n",
    "    y_pred = regr.predict(X_test)\n",
    "    MSE_k[k_ind] = ((y_test - y_pred) ** 2).mean()\n",
    "    k_ind += 1\n",
    "\n",
    "MSE_test = MSE_k.mean()\n",
    "MSE_test_std = MSE_k.std()\n",
    "print('LOOCV test MSE =', MSE_test)\n",
    "print('Std dev of the LOOCV test MSE =', MSE_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Bias versus variance\n",
    "Recall the test estimate MSE from the LOOCV of approximately 0.2868 and the MSE(LOOCV) standard error of about 0.0444. What happens to the estimated MSE and MSE standard error in the $k$-fold cross validation above as $k$ increases? Try values of $k=2, 10, 50, 100, 800$.\n",
    "\n",
    "Note that the LOOCV method has low bias (estimated on large number of data) but high variance (errors are based on one draw). In contrast, the $k$-fold method has more bias (estimated with less data) but lower variance. Each test set has more observations.\n",
    "\n",
    "* $k$-fold cross validation can often provide more accurate estimates of the test error rate.\n",
    "* $k$-fold is less computationally intensive\n",
    "* LOOCV has the least bias\n",
    "* LOOCV is the most computationally expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
